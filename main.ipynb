{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7985217d",
   "metadata": {},
   "source": [
    "Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8050312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\appul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\appul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa833e7",
   "metadata": {},
   "source": [
    "Load the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de1dfc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (34152, 2)\n",
      "Test shape: (19971, 1)\n",
      "   label                                               text\n",
      "0      0  donald trump sends out embarrassing new year‚s...\n",
      "1      0  drunk bragging trump staffer started russian c...\n",
      "2      0  sheriff david clarke becomes an internet joke ...\n",
      "3      0  trump is so obsessed he even has obama‚s name ...\n",
      "4      0  pope francis just called out donald trump duri...\n",
      "['label', 'text']\n"
     ]
    }
   ],
   "source": [
    "#Load the training data\n",
    "train=pd.read_csv(\"data/training_data_lowercase.csv\",\n",
    "                   sep=\"\\t\",\n",
    "                   header=None,\n",
    "                   names=[\"label\",\"text\"])\n",
    "\n",
    "#Load the testing data\n",
    "test=pd.read_csv(\"data/testing_data_lowercase_nolabels.csv\",\n",
    "                  header=None,\n",
    "                  names=[\"text\"])\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "print(train.head())\n",
    "print(train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7575000a",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa652d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cleaning text\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text=re.sub(\"r[^a-z\\s]\",\"\",text)\n",
    "    #Tokenize\n",
    "    tokens=word_tokenize(text)\n",
    "    #Stopwords\n",
    "    sw=set(stopwords.words(\"english\"))\n",
    "    tokens=[w for w in tokens if w not in sw]\n",
    "    #Lemmatization\n",
    "    lem=WordNetLemmatizer()\n",
    "    tokens=[lem.lemmatize(w) for w in tokens]\n",
    "    #Remove short tokens\n",
    "    tokens=[w for w in tokens if len(w)>2]\n",
    "    return\"\".join(tokens)\n",
    "\n",
    "print(\"\\n Cleaning text\")\n",
    "train[\"clean_text\"]=train[\"text\"].apply(clean_text)\n",
    "test[\"clean_text\"]=test[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3649bbfc",
   "metadata": {},
   "source": [
    "Split, vectorizing and naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8975c2fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      4\u001b[39m X_train,X_val,y_train,y_val=train_test_split(\n\u001b[32m      5\u001b[39m     X,y,test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m,stratify=y\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m vectorizer=TfidfVectorizer(\n\u001b[32m      9\u001b[39m     stop_words=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     ngram_range=(\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m),\n\u001b[32m     11\u001b[39m     max_features=\u001b[32m40000\u001b[39m,\n\u001b[32m     12\u001b[39m     min_df=\u001b[32m3\u001b[39m\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m X_train_tfidf=\u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m X_val_tfidf=vectorizer.transform(X_val)\n\u001b[32m     17\u001b[39m X_test_tfidf=vectorizer.transform(test[\u001b[33m\"\u001b[39m\u001b[33mclean_text\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\appul\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\appul\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\appul\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1368\u001b[39m             warnings.warn(\n\u001b[32m   1369\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m             )\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1379\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\appul\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1262\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1264\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1265\u001b[39m             feature_idx = vocabulary[feature]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\appul\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:104\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m         doc = \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    106\u001b[39m         doc = tokenizer(doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\appul\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:62\u001b[39m, in \u001b[36m_preprocess\u001b[39m\u001b[34m(doc, accent_function, lower)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03mapply to a document.\u001b[39;00m\n\u001b[32m     45\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m \u001b[33;03m    preprocessed string\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     doc = \u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m()\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m     doc = accent_function(doc)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "X=[\"clean_text\"]\n",
    "y=[\"label\"].astype(int)\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(\n",
    "    X,y,test_size=0.2, random_state=42,stratify=y\n",
    ")\n",
    "\n",
    "vectorizer=TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1,3),\n",
    "    max_features=40000,\n",
    "    min_df=3\n",
    ")\n",
    "\n",
    "X_train_tfidf=vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf=vectorizer.transform(X_val)\n",
    "X_test_tfidf=vectorizer.transform(test[\"clean_text\"])\n",
    "\n",
    "nb=MultinomialNB(alpha=1.0)\n",
    "nb.fit(X_train_tfidf,y_train)\n",
    "\n",
    "y_val_pred=nb.predict(X_val_tfidf)\n",
    "acc=accuracy_score(y_val,y_val_pred)\n",
    "\n",
    "print(f\"\\n Validation Accuracy:{acc:4f}\\n\")\n",
    "print(\"classification_report:\\n\", classification_report(y_val,y_val_pred, digits=4))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val,y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b7715",
   "metadata": {},
   "source": [
    "Vectorize with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d3443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_pipeline(text):\n",
    "    # Step 1: Tokenize the text\n",
    "    tokens= word_tokenize(text.lower())\n",
    "    # Step 2: Remove stop words\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    tokens=[token for token in tokens if token not in stop_words]\n",
    "    # Step 3: Remove punctuation\n",
    "    tokens=[token for token in tokens if token not in string.punctuation]\n",
    "    # Step 4: Apply lemmatization\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    lemmatized_tokens=[lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return lemmatized_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9ed67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model=\u001b[43mmake_pipeline\u001b[49m(\n\u001b[32m      2\u001b[39m     TfidfVectorizer(\n\u001b[32m      3\u001b[39m         lowercase=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      4\u001b[39m         stop_words=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m         ngram_range=(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m), \u001b[38;5;66;03m#try 1-3\u001b[39;00m\n\u001b[32m      6\u001b[39m         min_df=\u001b[32m2\u001b[39m,\n\u001b[32m      7\u001b[39m         max_df=\u001b[32m0.9\u001b[39m,\n\u001b[32m      8\u001b[39m         max_features=\u001b[32m50000\u001b[39m\n\u001b[32m      9\u001b[39m     ),\n\u001b[32m     10\u001b[39m     MultinomialNB(alpha=\u001b[32m0.5\u001b[39m) \u001b[38;5;66;03m#try 0.1 and 1.0\u001b[39;00m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m model.fit(X_train,y_train)\n",
      "\u001b[31mNameError\u001b[39m: name 'make_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "model=make_pipeline(\n",
    "    TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1,2), #try 1-3\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        max_features=50000\n",
    "    ),\n",
    "    MultinomialNB(alpha=0.5) #try 0.1 and 1.0\n",
    ")\n",
    "\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704295a6",
   "metadata": {},
   "source": [
    "Evaluate on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f8099",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_val)\n",
    "print(\"Validation accuracy:\", round(accuracy_score(y_val,y_pred),4))\n",
    "print(classification_report(y_val,y_pred,digits=3))\n",
    "print(confusion_matrix(y_val,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d93ffab",
   "metadata": {},
   "source": [
    "Now train on all data and predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f17e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain on full training data\n",
    "model.fit(train_df[\"text\"], train_df[\"label\"])\n",
    "\n",
    "#Predict labels for test\n",
    "test_pred=model.predict(test_df[\"text\"]).astype(int)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
