{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7985217d",
   "metadata": {},
   "source": [
    "Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\appul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\appul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\appul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa833e7",
   "metadata": {},
   "source": [
    "Load the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1dfc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (68307, 2)\n",
      "Test shape: (19971, 1)\n",
      "          label                                               text\n",
      "0  <<<<<<< HEAD                                               None\n",
      "1            ﻿0  donald trump sends out embarrassing new year‚s...\n",
      "2             0  drunk bragging trump staffer started russian c...\n",
      "3             0  sheriff david clarke becomes an internet joke ...\n",
      "4             0  trump is so obsessed he even has obama‚s name ...\n",
      "['label', 'text']\n"
     ]
    }
   ],
   "source": [
    "#Load the training data\n",
    "train=pd.read_csv(\"data/training_data_lowercase.csv\",\n",
    "                   sep=\"\\t\",\n",
    "                   header=None,\n",
    "                   names=[\"label\",\"text\"],\n",
    "                   engine=\"python\")\n",
    "\n",
    "#Load the testing data\n",
    "test=pd.read_csv(\"data/testing_data_lowercase_nolabels.csv\",\n",
    "                  header=None,\n",
    "                  names=[\"text\"])\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "print(train.head())\n",
    "print(train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7575000a",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #Remove urls and html\n",
    "    text.re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", text)\n",
    "    text=re.sub(r\"<.*?>\",\"\", text)\n",
    "    #Only alphabets and space\n",
    "    text=re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3649bbfc",
   "metadata": {},
   "source": [
    "Split, vectorizing and naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8975c2fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\appul\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[144]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m X=train_df[\u001b[33m\"\u001b[39m\u001b[33mclean_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m y=\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      4\u001b[39m X_train,X_val,y_train,y_val=train_test_split(\n\u001b[32m      5\u001b[39m     X,y,test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m,stratify=y\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m vectorizer=TfidfVectorizer(\n\u001b[32m      9\u001b[39m     stop_words=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     ngram_range=(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m),\n\u001b[32m     11\u001b[39m     max_features=\u001b[32m20000\u001b[39m,\n\u001b[32m     12\u001b[39m     min_df=\u001b[32m2\u001b[39m\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\appul\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\appul\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'label'"
     ]
    }
   ],
   "source": [
    "X=train_df[\"clean_text\"]\n",
    "y=train_df[\"label\"]\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(\n",
    "    X,y,test_size=0.2, random_state=42,stratify=y\n",
    ")\n",
    "\n",
    "vectorizer=TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1,2),\n",
    "    max_features=20000,\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "X_train_tfidf=vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf=vectorizer.transform(X_val)\n",
    "X_test_tfidf=vectorizer.transform(test_df[\"clean_text\"])\n",
    "\n",
    "nb=MultinomialNB(alpha=1.0)\n",
    "nb.fit(X_train_tfidf,y_train)\n",
    "\n",
    "y_val_pred=nb.predict(X_val_tfidf)\n",
    "acc=accuracy_score(y_val,y_val_pred)\n",
    "\n",
    "print(f\"\\n Validation Accuracy:{acc:4f}\\n\")\n",
    "print(\"classification_report:\\n\", classification_report(y_val,y_val_pred, digits=4))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39293657",
   "metadata": {},
   "source": [
    "Feature Extraction TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598420f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer=TfidfVectorizer()\n",
    "\n",
    "#Fit and transform the corpus into a TF-IDF representation\n",
    "X_tfidf=tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "#Show results\n",
    "print(\"TF-IDF:\\n\", X_tfidf.toarray())\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b7715",
   "metadata": {},
   "source": [
    "Vectorize with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d3443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_pipeline(text):\n",
    "    # Step 1: Tokenize the text\n",
    "    tokens= word_tokenize(text.lower())\n",
    "    # Step 2: Remove stop words\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    tokens=[token for token in tokens if token not in stop_words]\n",
    "    # Step 3: Remove punctuation\n",
    "    tokens=[token for token in tokens if token not in string.punctuation]\n",
    "    # Step 4: Apply lemmatization\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    lemmatized_tokens=[lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9ed67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      1\u001b[39m model=make_pipeline(\n\u001b[32m      2\u001b[39m     TfidfVectorizer(\n\u001b[32m      3\u001b[39m         lowercase=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     MultinomialNB(alpha=\u001b[32m0.5\u001b[39m) \u001b[38;5;66;03m#try 0.1 and 1.0\u001b[39;00m\n\u001b[32m     11\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m model.fit(\u001b[43mX_train\u001b[49m,y_train)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "model=make_pipeline(\n",
    "    TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1,2), #try 1-3\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        max_features=50000\n",
    "    ),\n",
    "    MultinomialNB(alpha=0.5) #try 0.1 and 1.0\n",
    ")\n",
    "\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704295a6",
   "metadata": {},
   "source": [
    "Evaluate on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f8099",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_val)\n",
    "print(\"Validation accuracy:\", round(accuracy_score(y_val,y_pred),4))\n",
    "print(classification_report(y_val,y_pred,digits=3))\n",
    "print(confusion_matrix(y_val,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d93ffab",
   "metadata": {},
   "source": [
    "Now train on all data and predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f17e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain on full training data\n",
    "model.fit(train_df[\"text\"], train_df[\"label\"])\n",
    "\n",
    "#Predict labels for test\n",
    "test_pred=model.predict(test_df[\"text\"]).astype(int)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
